#This file allows its internal values to be accessed by Snakefiles, as well as within python scripts called by rules. 


#It's treated as a python dictionary in Snakefiles and python scripts, and must be designated as the configfile by a Snakefile
#to be used by either. 

#This is done in Snakefiles with the declaration:
#configfile: </workdir/relative/path/to/name.yaml>

#which allows it to be called as config in the Snakefile, and snakemake.config in python scripts called by that snakefile. 


#The values stored are:

#1: HDX Timepoints in integer seconds
#2: Filenames corresponding to each timepoint (.RAW, .mzML, or .mzML.gz depending on what you're starting with)
#3: Important parameters which will constrain different functions within the pipeline



#Name for pipeline run:
"run_name": "2021_lib15_ph6"

#Path to .csv file where each row is a library protein name followed by its sequence
"names_and_seqs": "resources/0_names_seqs_masses/lib_9_name_seq_mass.csv"

########################################
###############TIMEPOINTS###############
########################################


#Enter your HDX timepoints in integer seconds, ascending, in .yaml list format:

#timepoints:
#  - 0
#  - 10
#  - 16
#  .
#  .
#  .
#  - tp_n

#Must start with 0 for undeuterated!

#in seconds, in ascending order, including 0
timepoints:
   - 0
   - 25
   - 38
   - 58
   - 90
   - 138
   - 211
   - 324
   - 498
   - 763
   - 1171
   - 1795



########################################
###############FILE PATHS###############
########################################


#Use the integer second timepoints as keys and enter all corresponding MS-Data-file filenames in list form (allows for replicates of any/all timepoints):
#0:
#  - <tp0_fn1>
#  - <tp0_fn2>
#10:
#  - <tp1_fn1>
#16:
#  - <tp2_fn1>
#  - <tp2_fn2>
#.
#.
#.
#tp_n: 
#  - <tpN_fn1>

#Exclude paths in filenames! 

#If you provide .mzML or .mzML.gz files the pipeline will recognize this and skip upstream jobs to start as far downstream as possible.

0:
  - "20211012_Lib09_pH6_0s_0.mzML"
  - "20211012_Lib09_pH6_0s_1.mzML"
25:
  - "20211012_Lib09_pH6_25s.mzML"
38:
  - "20211012_Lib09_pH6_38s.mzML"
58:
  - "20211012_Lib09_pH6_58s.mzML"
90:
  - "20211012_Lib09_pH6_90s.mzML"
138:
  - "20211012_Lib09_pH6_138s.mzML"
211:
  - "20211012_Lib09_pH6_211s.mzML"
324:
  - "20211012_Lib09_pH6_324s.mzML"
498:
  - "20211012_Lib09_pH6_498s.mzML"
763:
  - "20211012_Lib09_pH6_763s.mzML"
1171:
  - "20211012_Lib09_pH6_1171s.mzML"
1795:
  - "20211012_Lib09_pH6_1795s.mzML"


########################################
###############PARAMETERS###############
########################################


#Control parameters for various important functional values in the pipeline, format is <(str) name_of_param>: <value>; value datatype is specific to use within pipeline.

### PREPROCESSING ###
# Determines percentage of max LC peak an LC peak must reach to be included in creation of consensus RT bounds 
"lc_peak_prominence_cutoff": 0.15 
#Sets bounds on how far away a peak can be in LC from others like it and still be processed in a single DataTensor, default is 0.2 mins
"rt_group_cutoff": 0.2 

#mz calibration polyfit parameters
"polyfit_calibration": False
"polyfit_deg": 2
"ppm_tolerance": 50
"intensity_tolerance": 10000
"cluster_corr_tolerance": 0.99
"ppm_refilter": 10

# Set calibration from MZML file
# lockmass: (bool) True if calibration should be performed from mzml function 2
# lockmass compound: (str) which compound was used for lockmass. Choose from: SodiumFormate, GluFibPrecursor, GluFibFragments
# polyfit degree: (int) degree to use to polynomial fitting when generating calibration curves
# ms_resolution: (int) proxy for peak resolution on the ms instrument which is used when reprofiling peaks
# m0: (float) lower bound of the mz range acquired on the MS experiment
# m1: (float) upper bound of the mz range acquired on the MS experiment
# ppm_lockmass_radius: (float) window to consider when reprofiling peaks
# bins_per_isotopic_peak: (int) number of points to define the peak distribution. These points are linearly spaced on the ppm window range
# time_bins: (int) how many parts should the chromatography be divided to extract calibration curves from lockmass
"lockmass": True
"lockmass_compound": "SodiumFormate"
"polyfit_deg": 3
"ms_resolution": 12500
"m0": 280
"m1": 1600
"ppm_lockmass_radius": 50
"bins_per_isotopic_peak": 100
"runtime": 30
"time_bins": 5

### Main Snakefile ###

#path to old data for comparison, !!!set to null to make None in python!!! this will be default or removed from full release, requires ending '/' for dir
"old_data_dir": "/projects/p30802/digs_old/HX/180604_mix2_MES/hx_fits/"

#boundaries on tensor size in each dimension (LC-time in decimal minutes, IMS time in ms, M/Z by mass-units)
"rt_radius": 0.40
"dt_radius_scale": 0.06

#the two values below are added to the length of the protein sequence to determine the total_mass_window of a tensor (70 for a protein with 43 resiudes)

#this value creates a margin behind the POI monoisotopic mass to avoid IsotopicCluster truncation
"low_mass_margin": 10 
#this value is added to the length of the amino acid sequence to give a 'rightward' margin for readability 
"high_mass_margin": 17 

#error bounds on expected peak positions
"ppm_radius": 30

#Number of factors to use in the Non-Negative_PARAFAC decompositions of all DataTensors, ideally should be = (# of protein signals in DataTensor + 1 factor for noise)
#if n_factors_low = n_factors_high, only the low value is used, otherwise all intervening values are used, including the upper value. 2 - 5 will perform 4 factorizations. 
#use multiple factorizations to get better chances of a good factorization for each tensor. Beware the factorization is computationally intensive, increasing with n_factors
"n_factors_low": 3
"n_factors_high": 3

#used for smoothing coarse retention and drift time axes of DataTensors
#std-dev of rt dimension gaussian
#std_dev of dt dimension gaussian
"gauss_params": 
  - 3 
  - 1 

#0 for false, 1 for true
#Check 'isotopic dot product' of Undeuterated replicates for a charge,
#do not include in downstream if idotp is lower than cutoff
"idotp_filter": 1
#idotp < cutoff will not be considered for rt_groups
"idotp_cutoff": 0.99

#factor filter option. bool (True or False)
# set the r2 cutoff values for rt and dt data of the factors
"filter_factor": True
"factor_rt_r2_cutoff": 0.91
"factor_dt_r2_cutoff": 0.91


# ic generation options
# ic_peak_prominence: sets the prominence value for finding peaks in the intergated mz distribution
# ic_peak_width: sets with width value for finding peaks in the integrated mz distribution
# ic_rel_height_filter: bool (True or False). Whether to filter indices based on relative intensity of mz values within the peak
# ic_rel_height_filter_baseline: sets baseline threshold for relative height filtering
# ic_rel_height_threshold: sets the relative peak height to include based on the ht of the peak center
"ic_peak_prominence": 0.10
"ic_peak_width": 2
"auto_ic_peak_width": True
"ic_rel_height_filter": True
"ic_rel_height_filter_baseline": 0.10
"ic_rel_height_threshold": 0.10

# User weak pareto filter to retain only ics that are best in at least one criterion
"pareto_prefilter": True

# User defined thresholds to filter ics before weak_pareto_filter and optimize paths
"user_prefilter": True
"thresholds":
  "idotp_cutoff" : 0.99
  "baseline_peak_error": 20.
  "dt_ground_err": 20.
  "dt_ground_fit": 0.9
  "rt_ground_err": 20
  "rt_ground_fit": 0.9
  "baseline_integrated_rmse": 0.10
  "baseline_integrated_FWHM": 1.
  "nearest_neighbor_correlation": 0.     
    
     
#Limits number of plotted runner ICs in bokeh_plot
"n_runners": 5

# Run path optimizer again. This option will delete all ic_time_series pdfs.
"rerun-path-optimizer" : False

# Delete unnecessary files. This option will keep just essential files to rerun pathOptimizer.
"delete-files" : False
"keep_gz_mzml": True

     
     
